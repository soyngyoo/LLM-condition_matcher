{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82130e47-9cd5-474b-a04b-d7b43ae05a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from captions import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, get_linear_schedule_with_warmup\n",
    "import os \n",
    "\n",
    "# 각 c값에 대한 레벨 정의\n",
    "c1_levels = [0.0005, 0.005, 0.05, 0.5, 5]  # 5 levels\n",
    "c2_levels = [0.0, 0.1, 0.2, 0.3, 0.4]  # 5 levels\n",
    "c3_levels = [0.6, 0.7, 0.8, 0.9]  # 4 levels\n",
    "pp_levels = [4, 5, 6]  # 3 levels\n",
    "\n",
    "def generate_dataset(num_samples=20000):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    def shuffle_conditions(c1_text, c2_text, c3_text, pp_text):\n",
    "        conditions = [c1_text, c2_text, c3_text, pp_text]\n",
    "        random.shuffle(conditions)\n",
    "        return \" \".join(conditions)\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        c1 = random.choice(c1_levels)\n",
    "        c2 = random.choice(c2_levels)\n",
    "        c3 = random.choice(c3_levels)\n",
    "        pp = random.choice(pp_levels)\n",
    "        \n",
    "        # 각 조건에 대해 4개의 설명은 훈련 데이터로, 1개는 테스트 데이터로 사용\n",
    "        c1_train = random.sample(c1_descriptions[c1], 4)\n",
    "        c1_test = list(set(c1_descriptions[c1]) - set(c1_train))[0]\n",
    "        \n",
    "        c2_train = random.sample(c2_descriptions[c2], 4)\n",
    "        c2_test = list(set(c2_descriptions[c2]) - set(c2_train))[0]\n",
    "        \n",
    "        c3_train = random.sample(c3_descriptions[c3], 4)\n",
    "        c3_test = list(set(c3_descriptions[c3]) - set(c3_train))[0]\n",
    "        \n",
    "        pp_train = random.sample(pp_descriptions[pp], 2)  # pp는 3개 설명만 있으므로 2:1로 분할\n",
    "        pp_test = list(set(pp_descriptions[pp]) - set(pp_train))[0]\n",
    "        \n",
    "        # 훈련 데이터 생성\n",
    "        for _ in range(4):  # 각 조건 조합에 대해 4개의 샘플 생성\n",
    "            c1_text = random.choice(c1_train)\n",
    "            c2_text = random.choice(c2_train)\n",
    "            c3_text = random.choice(c3_train)\n",
    "            pp_text = random.choice(pp_train)\n",
    "            \n",
    "            combined_text = shuffle_conditions(c1_text, c2_text, c3_text, pp_text)\n",
    "            train_data.append({\n",
    "                'c1_value': c1_levels.index(c1),\n",
    "                'c2_value': c2_levels.index(c2),\n",
    "                'c3_value': c3_levels.index(c3),\n",
    "                'pp_value': pp_levels.index(pp),\n",
    "                'combined_text': combined_text\n",
    "            })\n",
    "        \n",
    "        # 테스트 데이터 생성\n",
    "        combined_text = shuffle_conditions(c1_test, c2_test, c3_test, pp_test)\n",
    "        test_data.append({\n",
    "            'c1_value': c1_levels.index(c1),\n",
    "            'c2_value': c2_levels.index(c2),\n",
    "            'c3_value': c3_levels.index(c3),\n",
    "            'pp_value': pp_levels.index(pp),\n",
    "            'combined_text': combined_text\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(train_data), pd.DataFrame(test_data)\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class ConditionTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.texts = df['combined_text'].to_numpy()\n",
    "        self.c1_values = df['c1_value'].to_numpy()\n",
    "        self.c2_values = df['c2_value'].to_numpy()\n",
    "        self.c3_values = df['c3_value'].to_numpy()\n",
    "        self.pp_values = df['pp_value'].to_numpy()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'c1_value': torch.tensor(self.c1_values[idx], dtype=torch.long),\n",
    "            'c2_value': torch.tensor(self.c2_values[idx], dtype=torch.long),\n",
    "            'c3_value': torch.tensor(self.c3_values[idx], dtype=torch.long),\n",
    "            'pp_value': torch.tensor(self.pp_values[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = ConditionTextDataset(df, tokenizer, max_len)\n",
    "    return DataLoader(dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "    \n",
    "# 데이터셋 생성\n",
    "train_df, test_df = generate_dataset(num_samples=2500)  # 20000 / 4 = 5000 (각 조합당 4개의 훈련 샘플을 생성하므로)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "train_df.to_csv('train_conditions_dataset.csv', index=False)\n",
    "test_df.to_csv('test_conditions_dataset.csv', index=False)\n",
    "\n",
    "# 사용 예시\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, max_len=128, batch_size=16)\n",
    "test_data_loader = create_data_loader(test_df, tokenizer, max_len=128, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b93ab3-5f08-4e2a-87f4-9ebb558d216f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c67186-13a5-4fb4-89bf-fe0e6d367061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super(ConditionClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        \n",
    "        # Shared layers\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        \n",
    "        # Separate output layers for each condition\n",
    "        self.fc_c1 = nn.Linear(hidden_dim // 2, 5)  # 5 levels for c1\n",
    "        self.fc_c2 = nn.Linear(hidden_dim // 2, 5)  # 5 levels for c2\n",
    "        self.fc_c3 = nn.Linear(hidden_dim // 2, 4)  # 4 levels for c3\n",
    "        self.fc_pp = nn.Linear(hidden_dim // 2, 3)  # 3 levels for pp\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        x = self.drop(pooled_output)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        \n",
    "        # Separate outputs for each condition\n",
    "        c1_output = self.fc_c1(x)\n",
    "        c2_output = self.fc_c2(x)\n",
    "        c3_output = self.fc_c3(x)\n",
    "        pp_output = self.fc_pp(x)\n",
    "        \n",
    "        return c1_output, c2_output, c3_output, pp_output\n",
    "\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples, num_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    max_train_steps = num_epochs * num_batches\n",
    "    progress_bar = tqdm(range(max_train_steps), desc=\"Steps\")\n",
    "    \n",
    "    for current_epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        total_steps = 0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            # Convert targets to long (class indices)\n",
    "            c1_labels = batch['c1_value'].to(device).long()\n",
    "            c2_labels = batch['c2_value'].to(device).long()\n",
    "            c3_labels = batch['c3_value'].to(device).long()\n",
    "            pp_labels = batch['pp_value'].to(device).long()\n",
    "\n",
    "            # Forward pass\n",
    "            c1_output, c2_output, c3_output, pp_output = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Calculate loss for each condition\n",
    "            loss_c1 = loss_fn(c1_output, c1_labels)\n",
    "            loss_c2 = loss_fn(c2_output, c2_labels)\n",
    "            loss_c3 = loss_fn(c3_output, c3_labels)\n",
    "            loss_pp = loss_fn(pp_output, pp_labels)\n",
    "            \n",
    "            total_loss = loss_c1 + loss_c2 + loss_c3 + loss_pp\n",
    "    \n",
    "            # Backward pass and optimization\n",
    "            total_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Accumulate loss\n",
    "            running_loss += total_loss.item()\n",
    "            losses.append(total_loss.item())\n",
    "            total_steps += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            logs = {\"step_loss\": total_loss.item(), \"lr\": scheduler.get_last_lr()[0]}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "        avg_loss = running_loss / total_steps\n",
    "        print(f\"Epoch {current_epoch+1}/{num_epochs} finished with avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "    save_model(model, optimizer, losses, OUTPUT_DIR, num_epochs)\n",
    "    return losses\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, all_losses, output_dir, epoch):\n",
    "    # 출력 디렉토리가 없으면 생성\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 모델 저장 경로 설정 (파일 확장자를 .pth로 설정)\n",
    "    model_save_path = os.path.join(output_dir, f\"model_epoch_{epoch}.pth\")\n",
    "    \n",
    "    # 체크포인트에 모델 상태, 옵티마이저 상태, 손실 값을 함께 저장\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),  # 선택사항: 옵티마이저 상태도 저장\n",
    "        'all_losses': all_losses,  # 손실 리스트 저장\n",
    "        'epoch': epoch  # 현재 에포크 저장\n",
    "    }\n",
    "    \n",
    "    # 체크포인트 저장\n",
    "    torch.save(checkpoint, model_save_path)\n",
    "    print(f\"Model and losses saved to {model_save_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5118c7e-94d9-47e0-82e7-454673777ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  50%|███████████████████████████▍                           | 624/1250 [00:53<00:53, 11.67it/s, lr=1e-5, step_loss=3.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 finished with avg loss: 4.7185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps: 100%|█████████████████████████████████████████████████████████| 1250/1250 [01:47<00:00, 11.68it/s, lr=0, step_loss=2.84]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 finished with avg loss: 3.1146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps: 100%|█████████████████████████████████████████████████████████| 1250/1250 [01:51<00:00, 11.17it/s, lr=0, step_loss=2.84]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and losses saved to output/model_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "RANDOM_SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 2e-5\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# Model initialization\n",
    "model = ConditionClassifier()\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Loss function (CrossEntropy for classification)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Training\n",
    "all_losses = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(train_df),\n",
    "    EPOCHS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4252bda-4f8b-4d2b-84a5-1ea28c4c6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def load_model(model, model_path, device):\n",
    "    \"\"\"\n",
    "    저장된 모델을 불러오는 함수.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): 불러올 모델의 클래스\n",
    "        model_path (str): 저장된 모델의 경로\n",
    "        device (torch.device): 모델을 불러올 디바이스 (CPU/GPU)\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: 불러온 모델\n",
    "    \"\"\"\n",
    "    # 저장된 체크포인트 불러오기\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    all_losses = checkpoint['all_losses']\n",
    "    #epoch = checkpoint['epoch']\n",
    "\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    return model, all_losses\n",
    "\n",
    "# 평가는 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e494d84f-2943-4c1d-b4a3-c9c4650f44cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, all_losses = load_model(model, './output/model_epoch_20.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9dd99-f7fa-42c4-8cae-ff5f58b7807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_loss(all_losses, epoch):\n",
    "    # 손실을 시각화\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(all_losses, label='Training Loss')\n",
    "    \n",
    "    # 그래프 타이틀 및 축 레이블 설정\n",
    "    plt.title(f'Training Loss over Epochs (up to Epoch {epoch})')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss(log scale)')\n",
    "    plt.legend()\n",
    "    \n",
    "    #plt.yscale('log')\n",
    "    # 그래프 보여주기\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_training_loss(all_losses, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135196f3-779f-4314-96a3-5b79e535c712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
